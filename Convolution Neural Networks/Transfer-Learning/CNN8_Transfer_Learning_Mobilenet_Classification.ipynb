{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Transfer learning in image classification**\n",
        "\n",
        "In this notebook transfer learning is used pre-trained model from google's Tensorflow Hub is taken and re-trained that on flowers dataset. \n",
        "\n",
        "Using pre-trained model saves lot of time and computational budget for new classification problem at hand"
      ],
      "metadata": {
        "id": "QCEgvKMWiiMQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0tGyGzvKfdyM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import PIL.Image as Image\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SHAPE = (224, 224)\n",
        "classifier = tf.keras.Sequential([\n",
        "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4\" , input_shape = IMAGE_SHAPE + (3,))\n",
        "])"
      ],
      "metadata": {
        "id": "i5JuN1V0gIli"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHznjmPcgaxP",
        "outputId": "f478f789-f080-43e8-accc-1332b58e1e08"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer (KerasLayer)    (None, 1001)              3540265   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,540,265\n",
            "Trainable params: 0\n",
            "Non-trainable params: 3,540,265\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LOAD DATASET"
      ],
      "metadata": {
        "id": "V80KxO_HhFqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url,  cache_dir='.', untar=True)\n",
        "# cache_dir indicates where to download data. I specified . which means current directory\n",
        "# untar true will unzip it"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0J1yXK-3gqYX",
        "outputId": "185ed72d-735a-411a-9ba7-1dfa04f02724"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
            "228813984/228813984 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6SekZ1PzhIvx",
        "outputId": "7e3e33fe-0dc1-48aa-fd3d-748803712808"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./datasets/flower_photos'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "data_dir = pathlib.Path(data_dir)\n",
        "data_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKxYM10KhLA-",
        "outputId": "900c95ca-dd6f-4a77-c391-dfd396ec4fe8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('datasets/flower_photos')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(data_dir.glob('*/*.jpg'))[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30L0PFgGhNeA",
        "outputId": "870148ed-9981-48a2-acfe-6ef6f521937e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('datasets/flower_photos/tulips/16139439153_fbdee29a10_n.jpg'),\n",
              " PosixPath('datasets/flower_photos/tulips/7166570828_7c26ca5766_n.jpg'),\n",
              " PosixPath('datasets/flower_photos/tulips/5674127693_1ddbd81097.jpg'),\n",
              " PosixPath('datasets/flower_photos/tulips/4300258119_b03f2f956e.jpg'),\n",
              " PosixPath('datasets/flower_photos/tulips/2426849837_baefd9a518_n.jpg')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
        "print(image_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eovFMro7hSGN",
        "outputId": "2a92ea80-177c-46f4-dd77-2feb1817f1c6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3670\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read flowers images from disk into numpy array using opencv"
      ],
      "metadata": {
        "id": "Hj-o42YJhUu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flowers_images_dict = {\n",
        "    'roses': list(data_dir.glob('roses/*')),\n",
        "    'daisy': list(data_dir.glob('daisy/*')),\n",
        "    'dandelion': list(data_dir.glob('dandelion/*')),\n",
        "    'sunflowers': list(data_dir.glob('sunflowers/*')),\n",
        "    'tulips': list(data_dir.glob('tulips/*')),\n",
        "}"
      ],
      "metadata": {
        "id": "Ram0pzSqhV-Z"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flowers_labels_dict = {\n",
        "    'roses': 0,\n",
        "    'daisy': 1,\n",
        "    'dandelion': 2,\n",
        "    'sunflowers': 3,\n",
        "    'tulips': 4,\n",
        "}"
      ],
      "metadata": {
        "id": "cLmXHb4fhXRa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create X and y i.e independent and dependent variables"
      ],
      "metadata": {
        "id": "hMFgJb-nhdt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = [], []\n",
        "\n",
        "for flower_name, images in flowers_images_dict.items():\n",
        "    for image in images:\n",
        "        img = cv2.imread(str(image))\n",
        "        resized_img = cv2.resize(img,(224,224))\n",
        "        X.append(resized_img)\n",
        "        y.append(flowers_labels_dict[flower_name])"
      ],
      "metadata": {
        "id": "G6eqmkiThZQA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(X)\n",
        "y = np.array(y)"
      ],
      "metadata": {
        "id": "Uwmfkp1uho7i"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train test split"
      ],
      "metadata": {
        "id": "07J6Bxakhqia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
      ],
      "metadata": {
        "id": "RBgH_U7VhpOl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing: scale images"
      ],
      "metadata": {
        "id": "4ElUw8Wvhvf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled = X_train / 255\n",
        "X_test_scaled = X_test / 255"
      ],
      "metadata": {
        "id": "CN2TDM3mhswg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now take pre-trained model and retrain it using flowers images"
      ],
      "metadata": {
        "id": "h4873b3Yh_nK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Vector** gives all layers other than the last layer"
      ],
      "metadata": {
        "id": "1OLyNxYxphhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor_model = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
        "\n",
        "pretrained_model_without_top_layer = hub.KerasLayer(feature_extractor_model, input_shape=(224, 224, 3), trainable=False)"
      ],
      "metadata": {
        "id": "3hCzYq7siBzK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_of_flowers = 5\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  pretrained_model_without_top_layer,\n",
        "  tf.keras.layers.Dense(num_of_flowers)\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iox-b9gmiE2f",
        "outputId": "b56a59c4-d36d-4753-8529-a841379c6014"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer_1 (KerasLayer)  (None, 1280)              2257984   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 5)                 6405      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,264,389\n",
            "Trainable params: 6,405\n",
            "Non-trainable params: 2,257,984\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "  optimizer=\"adam\",\n",
        "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "  metrics=['acc'])\n",
        "\n",
        "model.fit(X_train_scaled, y_train, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oEYNVrgiJQR",
        "outputId": "bc3baf09-d7e3-4966-ac6c-bdf3c19960b8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "86/86 [==============================] - 13s 41ms/step - loss: 0.8886 - acc: 0.6668\n",
            "Epoch 2/5\n",
            "86/86 [==============================] - 4s 41ms/step - loss: 0.4165 - acc: 0.8608\n",
            "Epoch 3/5\n",
            "86/86 [==============================] - 4s 41ms/step - loss: 0.3239 - acc: 0.8964\n",
            "Epoch 4/5\n",
            "86/86 [==============================] - 4s 41ms/step - loss: 0.2672 - acc: 0.9201\n",
            "Epoch 5/5\n",
            "86/86 [==============================] - 4s 41ms/step - loss: 0.2296 - acc: 0.9360\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe2071bb2b0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test_scaled,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psMFlQ3tiSi1",
        "outputId": "9cd46d60-f203-4b2c-e72a-7c17087598b5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29/29 [==============================] - 2s 54ms/step - loss: 0.4030 - acc: 0.8519\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4030470550060272, 0.8518518805503845]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix , classification_report\n",
        "import numpy as np\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "y_pred_classes = [np.argmax(element) for element in y_pred]\n",
        "\n",
        "print(\"Classification Report: \\n\", classification_report(y_test, y_pred_classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R56gu3v2iS2J",
        "outputId": "fcddc229-2b87-44a6-bec5-47d5775d5649"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29/29 [==============================] - 2s 40ms/step\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.84      0.84       176\n",
            "           1       0.90      0.84      0.87       154\n",
            "           2       0.89      0.92      0.90       226\n",
            "           3       0.75      0.88      0.81       150\n",
            "           4       0.86      0.78      0.82       212\n",
            "\n",
            "    accuracy                           0.85       918\n",
            "   macro avg       0.85      0.85      0.85       918\n",
            "weighted avg       0.86      0.85      0.85       918\n",
            "\n"
          ]
        }
      ]
    }
  ]
}